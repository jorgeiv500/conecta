---
title: "Unidad 1"
format: revealjs
scrollable: true
---

<h1><center>Unidad 1: An√°lisis Exploratorio de Datos (EDA)</center></h1>
<h2><center>OVA 1: Introducci√≥n al An√°lisis de Datos</center></h2>
<center><img src="https://cdn.midjourney.com/511e8fbc-ad7c-4c06-9e6b-2465f2d1eb89/0_1.png"     
width="300" height="200" /></center>

---

# 0. Conceptos b√°sicos üë®‚Äçüè´

- **An√°lisis exploratorio de datos (EDA):** Es el proceso de examinar y comprender los datos antes de aplicar t√©cnicas m√°s avanzadas de an√°lisis. Ayuda a descubrir patrones, identificar valores at√≠picos y obtener ideas iniciales sobre los datos.

- **Conjunto de datos:** Es la colecci√≥n de informaci√≥n que se utilizar√° para el an√°lisis. Puede estar en forma de una tabla, archivo CSV, hoja de c√°lculo, etc.

- **Variables:** Son las caracter√≠sticas o atributos que se registran en el conjunto de datos. Pueden ser num√©ricas (como edad o ingresos) o categ√≥ricas (como g√©nero o estado civil).


---

- **Estad√≠sticas descriptivas:** Son medidas utilizadas para resumir y describir los datos. Incluyen la media, mediana, moda, desviaci√≥n est√°ndar y percentiles.

- **Visualizaci√≥n de datos:** Es la representaci√≥n gr√°fica de los datos. Los gr√°ficos como histogramas, diagramas de dispersi√≥n y gr√°ficos de barras ayudan a comprender la distribuci√≥n y las relaciones entre las variables.

- **Valores faltantes:** Son valores que faltan en el conjunto de datos. Es importante identificarlos y decidir si se deben eliminar, reemplazar o imputar de alguna manera.


---

- **Valores at√≠picos:** Son valores que difieren significativamente del resto de los datos. Pueden indicar errores en la recopilaci√≥n de datos o ser puntos de datos genuinos pero inusuales. Los valores at√≠picos deben ser identificados y evaluados para determinar si deben ser excluidos o tratados de manera especial.

- **Correlaci√≥n:** Es una medida de la relaci√≥n entre dos variables. Indica c√≥mo var√≠an juntas. La correlaci√≥n puede ser positiva (ambas variables aumentan o disminuyen juntas) o negativa (una variable aumenta mientras que la otra disminuye).

- **Transformaci√≥n de datos:** Es el proceso de aplicar funciones matem√°ticas o estad√≠sticas a los datos para cambiar su escala o distribuci√≥n. Puede ser √∫til para normalizar los datos o reducir el sesgo.

- **Conclusiones preliminares:** Son los hallazgos y observaciones iniciales derivados del an√°lisis exploratorio de datos. Proporcionan una base para realizar an√°lisis m√°s profundos y formular preguntas de investigaci√≥n adicionales. 

---
# 1. Carga del Conjunto de Datos ‚å®Ô∏è

Nuestro primer paso es cargar el conjunto de datos en nuestro entorno. Para esta tarea, utilizamos la funci√≥n `read_csv` de pandas, que lee un archivo CSV y lo convierte en un DataFrame de pandas, una estructura de datos bidimensional que es flexible y poderosa.

```python
import pandas as pd
df = pd.read_csv('dataset.csv')
```

Adem√°s de `read_csv`, pandas proporciona una variedad de funciones para leer datos en diferentes formatos. Aqu√≠ hay algunos ejemplos:

1. **`read_excel`:** Esta funci√≥n se utiliza para leer un archivo Excel y convertirlo en un DataFrame. Es muy √∫til cuando los datos se almacenan en hojas de c√°lculo de Excel.
\centering
```python
df = pd.read_excel('dataset.xlsx')
```

2. **`read_json`:** Esta funci√≥n se utiliza para leer un archivo JSON. Los archivos JSON son muy comunes en la web y esta funci√≥n puede ser √∫til cuando se trabaja con APIs.
\centering
```python
df = pd.read_json('dataset.json')
```

3. **`read_sql`:** Esta funci√≥n se utiliza para leer los resultados de una consulta SQL directamente en un DataFrame de pandas. Es muy √∫til cuando se trabaja con bases de datos.
\centering
```python
from sqlalchemy import create_engine
engine = create_engine('sqlite:///:memory:')
df = pd.read_sql('SELECT * FROM my_table', engine)
```

4. **`read_html`:** Esta funci√≥n se utiliza para leer tablas HTML directamente en un DataFrame. Es √∫til para el web scraping.
\centering
```python
df = pd.read_html('http://www.webpage.com/sampledata.html')
```

5. **`read_clipboard`:** Esta funci√≥n se utiliza para leer el contenido del portapapeles directamente en un DataFrame. Es √∫til para copiar y pegar datos r√°pidamente en un script de Python.
\centering
```python
df = pd.read_clipboard()
```

Cada una de estas funciones tiene sus propios par√°metros para manejar diferentes situaciones, como saltar filas, parsear fechas, especificar tipos de datos, etc. Para obtener m√°s detalles, siempre se puede referir a la [documentaci√≥n oficial de pandas](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html).

Adem√°s de la funci√≥n `read_csv`, pandas ofrece una variedad de funciones para cargar diferentes tipos de conjuntos de datos. Por ejemplo, si nuestro conjunto de datos est√° en formato Excel, podemos usar la funci√≥n `read_excel`. Si est√° en formato JSON, podemos usar la funci√≥n `read_json`. Tambi√©n hay funciones para leer SQL, HTML, Stata, SAS, Google BigQuery, entre otros.

```python
# Para cargar un archivo Excel
df = pd.read_excel('dataset.xlsx')

# Para cargar un archivo JSON
df = pd.read_json('dataset.json')

# Para cargar desde una consulta SQL
from sqlalchemy import create_engine
engine = create_engine('sqlite:///:memory:')
df = pd.read_sql_query("SELECT * FROM my_table;", engine)
```

Estas funciones hacen que pandas sea extremadamente vers√°til en t√©rminos de los tipos de conjuntos de datos que puede manejar. Independientemente del formato de nuestros datos, es probable que pandas tenga una forma de cargarlos en un DataFrame.

Despu√©s de cargar nuestros datos, el siguiente paso ser√≠a explorarlos y limpiarlos si es necesario. Esto puede implicar eliminar duplicados, manejar valores faltantes, convertir tipos de datos, etc. Una vez que nuestros datos est√©n limpios y listos para analizar, podemos comenzar a realizar nuestra investigaci√≥n.

---

# 2. An√°lisis y Resumen Estad√≠stico üßÆ

Una vez que los datos est√°n cargados, es √∫til obtener un resumen estad√≠stico de los mismos. La funci√≥n `describe` de pandas proporciona estad√≠sticas descriptivas que resumen la tendencia central, la dispersi√≥n y la forma de la distribuci√≥n de un conjunto de datos, excluyendo los valores `NaN`.

```python
df.describe(include='all')
```
El m√©todo `df.describe()` genera un resumen estad√≠stico de las columnas. Por defecto, proporciona un resumen de las columnas num√©ricas. Si se incluye el par√°metro `include='all'`, tambi√©n proporcionar√° un resumen estad√≠stico de las columnas no num√©ricas.

El resumen estad√≠stico proporcionado por `df.describe()` incluye:

- `count`: N√∫mero total de valores no nulos en la columna.
- `mean`: Valor medio de la columna.
- `std`: Desviaci√≥n est√°ndar de la columna.
- `min`: Valor m√≠nimo en la columna.
- `25%`: Valor del primer cuartil.
- `50%`: Valor de la mediana o segundo cuartil.
- `75%`: Valor del tercer cuartil.
- `max`: Valor m√°ximo en la columna.

Para las columnas no num√©ricas, `df.describe()` proporciona:

- `count`: N√∫mero total de valores no nulos.
- `unique`: N√∫mero de valores √∫nicos.
- `top`: Valor m√°s com√∫n.
- `freq`: Frecuencia del valor m√°s com√∫n.

Este m√©todo es muy √∫til para obtener una visi√≥n r√°pida de los datos, especialmente en la fase de exploraci√≥n inicial del an√°lisis de datos.


---

# 3. Preprocesamiento de DatosüóëÔ∏è

El preprocesamiento de datos es un paso cr√≠tico en cualquier an√°lisis de datos. Implica limpiar los datos al eliminar o llenar los valores nulos y eliminar las filas o columnas que no son necesarias para el an√°lisis.



---

## 3.1. Preprocesamiento de Datos en Finanzas Computacionales

El preprocesamiento de datos es un paso cr√≠tico en cualquier an√°lisis de datos, y en particular en el campo de las finanzas computacionales. En este campo, los datos son la base de todas las decisiones que se toman.

> üìç‚Äã‚Äã  *Los datos sucios o mal manejados pueden llevar a decisiones err√≥neas, que pueden costar millones de d√≥lares.*

> üìç‚Äã‚Äã Por lo tanto, es esencial que cualquier cient√≠fico de datos que trabaje en finanzas computacionales comprenda c√≥mo preprocesar los datos de manera efectiva.

El preprocesamiento de datos implica limpiar los datos al eliminar o llenar los valores nulos y eliminar las filas o columnas que no son necesarias para el an√°lisis.

**Por ejemplo**, si estamos analizando los precios de las acciones, es posible que tengamos datos de varias fuentes diferentes. Algunas de estas fuentes pueden tener valores nulos para ciertos d√≠as, o pueden tener errores en los datos. Es importante identificar estos problemas y tratarlos adecuadamente.

El siguiente c√≥digo muestra c√≥mo podemos eliminar los valores nulos de nuestro DataFrame en Python:

```python
df_clean = df.dropna()
```

**Sin embargo,** simplemente eliminar los valores nulos puede no ser siempre la mejor soluci√≥n. En algunos casos, puede ser mejor llenar estos valores con un valor predeterminado, como el valor medio, mediano o el m√°s frecuente. Esto puede hacerse con el m√©todo `fillna()` en Python. Por ejemplo:

```python
df_clean = df.fillna(df.mean())
```

Adem√°s, en el preprocesamiento de datos, tambi√©n puede ser necesario eliminar ciertas filas o columnas que no son necesarias para el an√°lisis.

Por ejemplo, si estamos analizando los precios de las acciones, es posible que no necesitemos la columna que contiene el volumen de acciones negociadas. Podemos eliminar esta columna con el m√©todo `drop()` en Python:

```python
df_clean = df.drop(['Volume'], axis=1)
```
---

## Ejemplo

Aqu√≠ tienes un ejemplo con datos ficticios utilizando un DataFrame de pandas en Python.
Veamos un DataFrame de ejemplo que contenga algunos **valores nulos:**

```python
import pandas as pd
import numpy as np

data = {
    'Company':['GOOG', 'MSFT', 'AAPL', 'GOOG', 'MSFT', 'AAPL', np.nan],
    'Date':['2023-10-01', '2023-10-01', '2023-10-01', '2023-10-02', '2023-10-02', '2023-10-02', '2023-10-03'],
    'Close Price':[1500, 200, 125, 1520, 205, 130, np.nan],
    'Volume':[2000, 3000, 1500, 2100, 3100, 1600, np.nan]
}
df = pd.DataFrame(data)
print(df)
````

---

Asi es c√≥mo se ve el DataFrame:



```python
Company        Date  Close Price  Volume
0    GOOG  2023-10-01       1500.0  2000.0
1    MSFT  2023-10-01        200.0  3000.0
2    AAPL  2023-10-01        125.0  1500.0
3    GOOG  2023-10-02       1520.0  2100.0
4    MSFT  2023-10-02        205.0  3100.0
5    AAPL  2023-10-02        130.0  1600.0
6     NaN  2023-10-03          NaN     NaN
```



---

Ahora, eliminamos los valores nulos:

```python
df_clean = df.dropna()
print(df_clean)
```
Esto reemplazar√° el valor nulo en 'Close Price' y 'Volume' con la media de los valores existentes en esas columnas respectivamente.


```python
  Company        Date  Close Price  Volume
0    GOOG  2023-10-01       1500.0  2000.0
1    MSFT  2023-10-01        200.0  3000.0
2    AAPL  2023-10-01        125.0  1500.0
3    GOOG  2023-10-02       1520.0  2100.0
4    MSFT  2023-10-02        205.0  3100.0
5    AAPL  2023-10-02        130.0  1600.0
```


---


Finalmente, podemos eliminar la columna 'Volume' que no necesitamos para nuestro an√°lisis usando el m√©todo drop():


```python
df_clean = df.drop(['Volume'], axis=1)
print(df_clean)
```



---

Esto eliminar√° la √∫ltima fila que tiene valores nulos.

Sin embargo, en lugar de eliminar los valores nulos, podr√≠amos llenarlos con la media de cada columna:

```python
df_clean = df.fillna(df.mean())
print(df_clean)
```


```python
  Company        Date  Close Price       Volume
0    GOOG  2023-10-01  1500.000000  2000.000000
1    MSFT  2023-10-01   200.000000  3000.000000
2    AAPL  2023-10-01   125.000000  1500.000000
3    GOOG  2023-10-02  1520.000000  2100.000000
4    MSFT  2023-10-02   205.000000  3100.000000
5    AAPL  2023-10-02   130.000000  1600.000000
6     NaN  2023-10-03   613.333333  2216.666667
<ipython-input-3-5cee832ebd15>:1: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.
  df_clean = df.fillna(df.mean())
```




---

## 3.2. Funciones üë®‚Äçüíª

...para preprocesamiento y manipulacion


---

### 3.2.1. Manipulaci√≥n de Datos

La manipulaci√≥n de datos es otra tarea com√∫n en el an√°lisis de datos. Esto puede incluir cambiar el nombre de las columnas, reordenar las columnas, etc. Para cambiar el nombre de las columnas, usamos la funci√≥n `rename`, y para reordenar las columnas, simplemente seleccionamos las columnas en el orden que deseamos.

```python
df.rename(columns={'old_name': 'new_name'}, inplace=True)
df = df[['col1', 'col2', 'col3']]
```


---

El c√≥digo realiza dos operaciones en un DataFrame:

- **Renombrar columnas:** df.rename(columns={'old_name': 'new_name'}, inplace=True) cambia el nombre de la columna 'old_name' a 'new_name'. Despu√©s de esta operaci√≥n, la columna que se llamaba 'old_name' ahora se llamar√° 'new_name'.

- **Reordenar columnas:** df = df[['col1', 'col2', 'col3']] esto reordena las columnas del DataFrame a 'col1', 'col2', 'col3' en ese orden. Despu√©s de esta operaci√≥n, la primera columna del DataFrame ser√° 'col1', la segunda columna ser√° 'col2', y la tercera columna ser√° 'col3'.




---

Aqu√≠ vemos una representaci√≥n gr√°fica de estas operaciones:


Inicialmente, el DataFrame podr√≠a verse as√≠:

| col1 | old_name | col3 | col4 |
|------|----------|------|------|
| ...  | ...      | ...  | ...  |

Despu√©s de renombrar la columna 'old_name' a 'new_name' con el c√≥digo `df.rename(columns={'old_name': 'new_name'}, inplace=True)`, tu DataFrame se ver√≠a as√≠:

| col1 | new_name | col3 | col4 |
|------|----------|------|------|
| ...  | ...      | ...  | ...  |

A continuaci√≥n, reordenando las columnas con el c√≥digo `df = df[['col1', 'col2', 'col3']]`, tu DataFrame cambiar√≠a a esto:

| col1 | col2 | col3 |
|------|------|------|
| ...  | ...  | ...  |

---

### Ejemplo

Ejemplo con datos ficticios de acciones:

Sup√≥n que tienes el siguiente DataFrame que representa los precios de cierre y el volumen de acciones negociadas para algunas empresas tech en fechas espec√≠ficas:



Tu DataFrame original se ver√≠a as√≠:

| Company | Date       | Old_Close_Price | Volume |
|---------|------------|-----------------|--------|
| GOOG    | 2023-10-01 | 1500            | 2000   |
| MSFT    | 2023-10-01 | 200             | 3000   |
| AAPL    | 2023-10-01 | 125             | 1500   |
| GOOG    | 2023-10-02 | 1520            | 2100   |
| MSFT    | 2023-10-02 | 205             | 3100   |
| AAPL    | 2023-10-02 | 130             | 1600   |

Si renombras la columna 'Old_Close_Price' a 'Close_Price' con el c√≥digo `df.rename(columns={'Old_Close_Price': 'Close_Price'}, inplace=True)`, tu DataFrame se ver√≠a as√≠:

| Company | Date       | Close_Price | Volume |
|---------|------------|-------------|--------|
| GOOG    | 2023-10-01 | 1500        | 2000   |
| MSFT    | 2023-10-01 | 200         | 3000   |
| AAPL    | 2023-10-01 | 125         | 1500   |
| GOOG    | 2023-10-02 | 1520        | 2100   |
| MSFT    | 2023-10-02 | 205         | 3100   |
| AAPL    | 2023-10-02 | 130         | 1600   |

Finalmente, si reordenas las columnas a 'Company', 'Close_Price', y 'Date' con el c√≥digo `df = df[['Company', 'Close_Price', 'Date']]`, tu DataFrame cambiar√≠a a esto:

| Company | Close_Price | Date       |
|---------|-------------|------------|
| GOOG    | 1500        | 2023-10-01 |
| MSFT    | 200         | 2023-10-01 |
| AAPL    | 125         | 2023-10-01 |
| GOOG    | 1520        | 2023-10-02 |
| MSFT    | 205         | 2023-10-02 |
| AAPL    | 130         | 2023-10-02 |

---


### 3.2.2. Data Wrangling

Data wrangling o munging es el proceso de convertir los datos del formato inicial a un formato que puede ser mejor analizado. Esto puede implicar convertir los tipos de datos, ordenar los datos, etc. Para convertir los tipos de datos, usamos la funci√≥n `astype` y para ordenar los datos, usamos la funci√≥n `sort_values`.

```python
df['column'] = df['column'].astype('type')
df.sort_values(by='column', ascending=True)
```




---

### Ejemplo

Supongamos que tienes el siguiente DataFrame que representa los precios de cierre y el volumen de acciones negociadas para algunas empresas tech en fechas espec√≠ficas:

```python
import pandas as pd

data = {
    'Company':['GOOG', 'MSFT', 'AAPL', 'GOOG', 'MSFT', 'AAPL'],
    'Date':['2023-10-01', '2023-10-01', '2023-10-01', '2023-10-02', '2023-10-02', '2023-10-02'],
    'Close_Price':['1500', '200', '125', '1520', '205', '130'], # Los precios est√°n en formato string
    'Volume':[2000, 3000, 1500, 2100, 3100, 1600]
}

df = pd.DataFrame(data)
```

El DataFrame original se ve as√≠:

| Company | Date       | Close_Price | Volume |
|---------|------------|-------------|--------|
| GOOG    | 2023-10-01 | "1500"      | 2000   |
| MSFT    | 2023-10-01 | "200"       | 3000   |
| AAPL    | 2023-10-01 | "125"       | 1500   |
| GOOG    | 2023-10-02 | "1520"      | 2100   |
| MSFT    | 2023-10-02 | "205"       | 3100   |
| AAPL    | 2023-10-02 | "130"       | 1600   |

Observa que los precios de cierre (`Close_Price`) est√°n en formato string. Para facilitar los an√°lisis futuros, podr√≠a ser √∫til convertir estos valores a n√∫meros. Podemos hacerlo utilizando la funci√≥n `astype`:

```python
df['Close_Price'] = df['Close_Price'].astype('int')
```

Despu√©s de convertir los tipos de datos, el DataFrame se ve as√≠:

| Company | Date       | Close_Price | Volume |
|---------|------------|-------------|--------|
| GOOG    | 2023-10-01 | 1500        | 2000   |
| MSFT    | 2023-10-01 | 200        | 3000   |
| AAPL    | 2023-10-01 | 125         | 1500   |
| GOOG    | 2023-10-02 | 1520        | 2100   |
| MSFT    | 2023-10-02 | 205         | 3100   |
| AAPL    | 2023-10-02 | 130         | 1600   |

A continuaci√≥n, podr√≠as querer ordenar los datos por precio de cierre en orden ascendente. Esto se puede hacer utilizando la funci√≥n `sort_values`:

```python
df = df.sort_values(by='Close_Price', ascending=True)
```

Despu√©s de ordenar los datos, el DataFrame se ve as√≠:

| Company | Date       | Close_Price | Volume |
|---------|------------|-------------|--------|
| AAPL    | 2023-10-01 | 125         | 1500   |
| AAPL    | 2023-10-02 | 130         | 1600   |
| MSFT    | 2023-10-01 | 200         | 3000   |
| MSFT    | 2023-10-02 | 205         | 3100   |
| GOOG    | 2023-10-01 | 1500        | 2000   |
| GOOG    | 2023-10-02 | 1520        | 2100   |

Espero que esto te d√© una buena idea de c√≥mo se pueden usar `astype` y `sort_values` en el proceso de data wrangling.

---

### 3.2.3. Agregaci√≥n de Datos por Categor√≠a

La agregaci√≥n de datos es un proceso en el que recopilamos y presentamos los datos de manera resumida para obtener informaci√≥n adicional. Esto implica agrupar los datos en funci√≥n de ciertas categor√≠as y realizar operaciones en estos grupos. Para agrupar los datos, usamos la funci√≥n `groupby`.

```python
df_grouped = df.groupby('column')
df_grouped.sum()
```


---


###3.2.4. Concatenar y Apendizar Data Sets

A veces, es posible que tengamos datos distribuidos en varios DataFrames. Podemos concatenar estos DataFrames en uno solo usando la funci√≥n `concat`.

```python
df_concat = pd.concat([df_white, df_red])
```
----------------------------------------

---


###3.2.5. Joins de Datasets

Finalmente, podemos combinar dos DataFrames bas√°ndonos en una columna com√∫n utilizando la funci√≥n `merge`. Hay varios tipos de joins como inner join, left join, right join y outer join.

```python
df_merged = pd.merge(df1, df2, on='key', how='inner')
```



---


### 3.2.6. Eliminaci√≥n de valores nulos

La funci√≥n `dropna()` se utiliza para eliminar los valores nulos (NaN) de un DataFrame. El DataFrame resultante, `df_clean`, no contendr√° ninguna fila que tuviera al menos un valor nulo.

```python
df_clean = df.dropna()
```



---

### 3.2.7 Renombrar y reordenar columnas

La funci√≥n `rename()` se utiliza para cambiar el nombre de las columnas. En este caso, cambiamos el nombre de la columna 'old_name' a 'new_name'. El argumento `inplace=True` realiza los cambios en el DataFrame original.

La reordenaci√≥n de las columnas se realiza simplemente seleccionando las columnas en el orden deseado.

```python
df.rename(columns={'old_name': 'new_name'}, inplace=True)
df = df[['col1', 'col2', 'col3']]
```


---

### 3.2.8. Conversi√≥n de tipos de datos y ordenaci√≥n

La funci√≥n `astype()` se utiliza para convertir los tipos de datos de las columnas. Aqu√≠, estamos cambiando el tipo de datos de la columna 'column' a 'type'.

La funci√≥n `sort_values()` se utiliza para ordenar los valores de una columna. En este caso, estamos ordenando los datos por la columna 'column' en orden ascendente.

```python
df['column'] = df['column'].astype('type')
df.sort_values(by='column', ascending=True)
```


---

### 3.2.9. Concatenaci√≥n de DataFrames

La funci√≥n `concat()` se utiliza para concatenar dos o m√°s DataFrames en uno solo. Aqu√≠, estamos concatenando los DataFrames `df_white` y `df_red`.

```python
df_concat = pd.concat([df_white, df_red])
```


---



---------------------------------------

üêç Estas son algunas de las funciones m√°s comunes que se utilizan en el preprocesamiento y la manipulaci√≥n de datos en pandas. Practicar con estas funciones te ayudar√° a adquirir habilidades s√≥lidas en el manejo de datos en Python.üêç

--------------------------

[Volver al Men√∫](#Men√∫)

---

## 4. Visualizaci√≥n

La visualizaci√≥n de datos es una parte importante del an√°lisis de datos. Nos ayuda a entender los patrones, tendencias y relaciones entre diferentes variables en nuestros datos.

La biblioteca matplotlib en Python es una herramienta poderosa para la visualizaci√≥n de datos. Aqu√≠ se muestran algunos ejemplos de c√≥mo visualizar datos en Python.

```python
import matplotlib.pyplot as plt
```

**Histograma:**
Un histograma es una representaci√≥n gr√°fica que organiza un grupo de datos en una serie de intervalos. Es √∫til para entender la distribuci√≥n de los datos.

```python
df['column'].hist(bins=10)
plt.show()
```

**Boxplot:**
Un boxplot es una forma estandarizada de representar la distribuci√≥n de los datos basada en un resumen de cinco n√∫meros (‚Äúm√≠nimo‚Äù, primer cuartil (Q1), mediana, tercer cuartil (Q3) y ‚Äúm√°ximo‚Äù).

```python
df.boxplot(column='column')
plt.show()
```

**Gr√°fico de dispersi√≥n:**
Un gr√°fico de dispersi√≥n utiliza puntos cartesianos para mostrar los valores de dos variables num√©ricas, lo que permite observar la correlaci√≥n entre las variables.

```python
plt.scatter(df['column1'], df['column2'])
plt.show()
```

**Gr√°fico de barras:**
Un gr√°fico de barras es √∫til para comparar cantidades de diferentes categor√≠as.

```python
df['column'].value_counts().plot.bar()
plt.show()
```

**Correlaci√≥n:**
La correlaci√≥n es una medida de la relaci√≥n entre dos variables. Podemos usar el m√©todo `corr` para calcular la correlaci√≥n entre todas las columnas num√©ricas en el DataFrame.

```python
correlation = df.corr()
```

**Heatmap:**
Un mapa de calor es una forma de visualizar la matriz de correlaci√≥n, donde los colores representan los valores de correlaci√≥n entre las variables.

```python
import seaborn as sns
sns.heatmap(correlation)
plt.show()
```

Estas son solo algunas de las muchas formas en que puedes visualizar tus datos en Python.

> üö® La elecci√≥n de las visualizaciones depender√° de tu conjunto de datos y de las preguntas que est√©s tratando de responder.

-----------------------------

[Volver al Men√∫](#Men√∫)

---

# 5. Ejemplo A

Con la intenci√≥n de proveer un desarrollo paso a paso, trabajaremos con un conjunto que se encuentra a su disposici√≥n. Nuestro conjunto se llama ***credito.csv:***

## 5.1. Descripci√≥n General

El conjunto de datos ha sido adaptado al contexto colombiano y traducido al espa√±ol, facilitando as√≠ su an√°lisis en este entorno espec√≠fico.

**Columnas del Conjunto de Datos**

- **Ingresos**: Representa los ingresos de los individuos en miles de d√≥lares.
- **Limite_Credito**: El l√≠mite de cr√©dito en d√≥lares asignado a los individuos.
- **Calificacion**: La calificaci√≥n crediticia de los individuos.
- **Tarjetas**: El n√∫mero de tarjetas de cr√©dito que tienen los individuos.
- **Edad**: La edad de los individuos en a√±os.
- **Educacion**: El nivel educativo de los individuos, medido en a√±os de educaci√≥n formal.
- **Propietario**: Indica si los individuos son propietarios de una casa (S√≠/No).
- **Estudiante**: Indica si los individuos son estudiantes (S√≠/No).
- **Casado**: Indica el estado civil de los individuos (S√≠/No).
- **Region**: La regi√≥n geogr√°fica de Colombia en la que residen los individuos. Las opciones incluyen 'Andina', 'Caribe', 'Pac√≠fica', 'Orinoqu√≠a', y 'Amazon√≠a'.
- **Saldo**: El saldo actual en las cuentas de los individuos en d√≥lares.

**Datos Faltantes**

Para simular una situaci√≥n m√°s realista, se han insertado valores nulos en un 5% de las entradas para las columnas 'Ingresos', 'Limite_Credito', 'Calificacion', y 'Saldo'.

**Recomendaciones**

Se recomienda realizar una limpieza y un preprocesamiento adecuado de los datos antes de utilizarlos para cualquier an√°lisis o modelado posterior.

Vamos a importar los datos.
Utilizaremos pandas para importar los datos y `df.head()` para leer el conjunto.

---

## Carga de datos

---

```python
import pandas as pd

df_credito=pd.read_csv('credito.csv')
df_credito.head()
```

---

## Resumen Estad√≠stico

Una vez que hemos importado los datos, el siguiente paso es obtener un resumen estad√≠stico para entender las caracter√≠sticas num√©ricas del conjunto de datos. Esto incluye medidas como la media, la mediana, la desviaci√≥n est√°ndar, entre otros.

---

```python
estadisticas = df_credito.describe(include='all')
estadisticas
```

---

## Resumen Estad√≠stico del Conjunto de Datos

### Variables Num√©ricas

- Ingresos: Var√≠a entre 10.35 y 186.63 con una media de aproximadamente 44.68.
- Limite_Credito: Rango entre 855 y 13,913, con una media de aproximadamente 4,739.
- Calificacion: Var√≠a entre 93 y 982, con una media de aproximadamente 357.
- Tarjetas: Var√≠a entre 1 y 9 tarjetas, con una media de aproximadamente 2.96.
- Edad: Rango de edad entre 23 y 98 a√±os, con una media de aproximadamente 55.67.
- Educacion: Nivel de educaci√≥n var√≠a entre 5 y 20, con una media de aproximadamente 13.45.
- Saldo: Var√≠a entre 0 y 1,999, con una media de aproximadamente 522.46.

### Variables Categ√≥ricas

- Propietario: Dos categor√≠as, siendo la m√°s frecuente "S√≠".
- Estudiante: Dos categor√≠as, siendo la m√°s frecuente "No".
- Casado: Dos categor√≠as, siendo la m√°s frecuente "S√≠".
- Regi√≥n: Cinco categor√≠as diferentes, siendo la m√°s frecuente "Pac√≠fica".

El siguiente paso es el preprocesamiento de los datos, que incluir√° la eliminaci√≥n de valores nulos y vac√≠os. Vamos a ello.

---

## Preprocesamiento de datos:

Una vez que hemos importado los datos, el siguiente paso es obtener un resumen estad√≠stico para entender las caracter√≠sticas num√©ricas del conjunto de datos. Esto incluye medidas como la media, la mediana, la desviaci√≥n est√°ndar, entre otros.

---

```python
# Identificar columnas con valores nulos
valores_faltantes = df_credito.isnull().sum()
valores_faltantes

```

---

Tenemos valores faltantes en varias columnas:

* Ingresos: 20 valores faltantes
* Limite_Credito: 20 valores faltantes
* Calificacion: 20 valores faltantes
* Saldo: 20 valores faltantes

vamos a eliminarlas:

---

```python
# Eliminar filas con valores nulos
df_limpio = df_credito.dropna()

# Verificar que se han eliminado todos los valores nulos
valores_faltantes_limpio = df_limpio.isnull().sum()
valores_faltantes_limpio, df_limpio.shape

```

---

## 4. Visualizaci√≥n

---

Una forma es usar matplotlib.

---

```python
import matplotlib.pyplot as plt
df_new=df_limpio
# Creando subplots
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(14, 12))

# Gr√°fico de barras para 'Propietario'
prop_counts = df_new['Propietario'].value_counts()
axes[0, 0].bar(prop_counts.index, prop_counts.values, color='b', alpha=0.7, label='Propietario')
axes[0, 0].set_title('Distribuci√≥n de Propietarios y No Propietarios')
axes[0, 0].set_xlabel('Propietario')
axes[0, 0].set_ylabel('Cantidad')

# Gr√°fico de barras para 'Estudiante'
est_counts = df_new['Estudiante'].value_counts()
axes[0, 1].bar(est_counts.index, est_counts.values, color='g', alpha=0.7, label='Estudiante')
axes[0, 1].set_title('Distribuci√≥n de Estudiantes y No Estudiantes')
axes[0, 1].set_xlabel('Estudiante')
axes[0, 1].set_ylabel('Cantidad')

# Gr√°fico de barras para 'Casado'
cas_counts = df_new['Casado'].value_counts()
axes[1, 0].bar(cas_counts.index, cas_counts.values, color='r', alpha=0.7, label='Casado')
axes[1, 0].set_title('Distribuci√≥n de Casados y Solteros')
axes[1, 0].set_xlabel('Casado')
axes[1, 0].set_ylabel('Cantidad')

# Gr√°fico de barras para 'Region'
reg_counts = df_new['Region'].value_counts()
axes[1, 1].bar(reg_counts.index, reg_counts.values, color='m', alpha=0.7, label='Region')
axes[1, 1].set_title('Distribuci√≥n por Regi√≥n')
axes[1, 1].set_xlabel('Regi√≥n')
axes[1, 1].set_ylabel('Cantidad')

# Histograma para 'Ingresos'
axes[2, 0].hist(df_new['Ingresos'].dropna(), bins=20, color='c', alpha=0.7, label='Ingresos')
axes[2, 0].set_title('Distribuci√≥n de Ingresos')
axes[2, 0].set_xlabel('Ingresos')
axes[2, 0].set_ylabel('Frecuencia')

# Histograma para 'Limite_Credito'
axes[2, 1].hist(df_new['Limite_Credito'].dropna(), bins=20, color='y', alpha=0.7, label='Limite_Credito')
axes[2, 1].set_title('Distribuci√≥n de L√≠mite de Cr√©dito')
axes[2, 1].set_xlabel('L√≠mite de Cr√©dito')
axes[2, 1].set_ylabel('Frecuencia')

# Ajustando la disposici√≥n
plt.tight_layout()
plt.show()

```

---

```python
import seaborn as sns

# Configurando el estilo de los gr√°ficos
sns.set(style="whitegrid")

# Creando subplots
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(14, 12))

# Gr√°fico de barras para 'Propietario'
sns.countplot(x='Propietario', data=df_new, ax=axes[0, 0])
axes[0, 0].set_title('Distribuci√≥n de Propietarios y No Propietarios')

# Gr√°fico de barras para 'Estudiante'
sns.countplot(x='Estudiante', data=df_new, ax=axes[0, 1])
axes[0, 1].set_title('Distribuci√≥n de Estudiantes y No Estudiantes')

# Gr√°fico de barras para 'Casado'
sns.countplot(x='Casado', data=df_new, ax=axes[1, 0])
axes[1, 0].set_title('Distribuci√≥n de Casados y Solteros')

# Gr√°fico de barras para 'Region'
sns.countplot(x='Region', data=df_new, ax=axes[1, 1])
axes[1, 1].set_title('Distribuci√≥n por Regi√≥n')

# Histograma para 'Ingresos'
sns.histplot(df_new['Ingresos'], kde=True, ax=axes[2, 0])
axes[2, 0].set_title('Distribuci√≥n de Ingresos')

# Histograma para 'Limite_Credito'
sns.histplot(df_new['Limite_Credito'], kde=True, ax=axes[2, 1])
axes[2, 1].set_title('Distribuci√≥n de L√≠mite de Cr√©dito')

# Ajustando la disposici√≥n
plt.tight_layout()
plt.show()

```

---

## An√°lisis Exploratorio de Datos (EDA) - Visualizaciones

### Distribuci√≥n de Propietarios y No Propietarios

La mayor√≠a de las personas en el conjunto de datos no son propietarias de una casa.

### Distribuci√≥n de Estudiantes y No Estudiantes

La mayor√≠a de las personas en el conjunto de datos no son estudiantes.

### Distribuci√≥n de Casados y Solteros

La mayor√≠a de las personas en el conjunto de datos est√°n casadas.

### Distribuci√≥n por Regi√≥n

La distribuci√≥n de individuos entre las diferentes regiones de Colombia parece ser aproximadamente uniforme.

### Distribuci√≥n de Ingresos

Los ingresos est√°n distribuidos en su mayor√≠a en el rango inferior, con algunos outliers en el extremo superior.

### Distribuci√≥n de L√≠mite de Cr√©dito

Similar a los ingresos, el l√≠mite de cr√©dito tambi√©n est√° en su mayor√≠a distribuido en el rango inferior, con algunos outliers en el extremo superior.

---

```python
# Boxplot para 'Ingresos' agrupado por 'Estudiante' usando Matplotlib
plt.figure(figsize=(8, 6))
data_to_plot = [df_new[df_new['Estudiante'] == 'Si']['Ingresos'].dropna(),
                df_new[df_new['Estudiante'] == 'No']['Ingresos'].dropna()]

plt.boxplot(data_to_plot, labels=['Estudiante', 'No Estudiante'], vert=True, patch_artist=True)
plt.title('Boxplot de Ingresos Agrupado por Estudiante')
plt.xlabel('Estado del Estudiante')
plt.ylabel('Ingresos (Miles de d√≥lares)')
plt.show()

```

---

### Boxplot de Ingresos Agrupado por Estado de Estudiante

Esta gr√°fica muestra la distribuci√≥n de ingresos separada por si el individuo es estudiante o no. Se observan diferencias notables en la distribuci√≥n de ingresos entre los dos grupos:

- Estudiantes: Los ingresos de los estudiantes est√°n concentrados en un rango mucho m√°s bajo en comparaci√≥n con los no estudiantes. Sin embargo, hay algunos valores at√≠picos que son considerablemente altos.

- No Estudiantes: La distribuci√≥n de ingresos para los no estudiantes es m√°s amplia y tambi√©n presenta valores at√≠picos en el extremo superior.

Esto podr√≠a sugerir que, aunque los estudiantes tienden a tener ingresos m√°s bajos, hay excepciones en las que algunos estudiantes tienen ingresos significativamente altos.

---

```python
# Calculando la matriz de correlaci√≥n
correlation_matrix = df_new.corr()

# Creando un heatmap para visualizar la matriz de correlaci√≥n
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Heatmap de Correlaci√≥n entre Variables Num√©ricas')
plt.show()

```

---

## Heatmap de Correlaci√≥n entre Variables Num√©ricas
El mapa de calor muestra la correlaci√≥n entre las variables num√©ricas del conjunto de datos. Aqu√≠ hay algunas observaciones clave:

- Ingresos y L√≠mite de Cr√©dito: Hay una fuerte correlaci√≥n positiva (0.79) entre los ingresos y el l√≠mite de cr√©dito, lo cual es esperable, ya que las personas con mayores ingresos suelen tener un mayor l√≠mite de cr√©dito.

- Calificaci√≥n y L√≠mite de Cr√©dito: Tambi√©n hay una fuerte correlaci√≥n positiva (0.99) entre la calificaci√≥n y el l√≠mite de cr√©dito. Esto sugiere que un l√≠mite de cr√©dito m√°s alto suele estar asociado con una mejor calificaci√≥n crediticia.

- Saldo y L√≠mite de Cr√©dito: Existe una correlaci√≥n positiva moderada (0.51) entre el saldo y el l√≠mite de cr√©dito.

- Edad y Tarjetas: Hay una correlaci√≥n positiva moderada (0.45) entre la edad y el n√∫mero de tarjetas de cr√©dito, lo cual podr√≠a indicar que las personas mayores tienden a tener m√°s tarjetas de cr√©dito.

El resto de las correlaciones son relativamente bajas, lo que sugiere que esas variables no est√°n fuertemente relacionadas entre s√≠.

---

### Conclusi√≥n del An√°lisis Exploratorio de Datos (EDA)

El An√°lisis Exploratorio de Datos en este conjunto, adaptado al contexto colombiano y traducido al espa√±ol, ha revelado varias observaciones y tendencias clave:

1. **Distribuciones Desbalanceadas**: Las variables categ√≥ricas como 'Propietario', 'Estudiante' y 'Casado' muestran distribuciones desbalanceadas. Por ejemplo, hay una mayor cantidad de no propietarios y no estudiantes en el conjunto.

2. **Ingresos y L√≠mites de Cr√©dito**: Estas variables muestran una correlaci√≥n fuerte y positiva. Los individuos con mayores ingresos tienden a tener un l√≠mite de cr√©dito m√°s alto.

3. **Calificaci√≥n Crediticia**: Esta variable tambi√©n muestra una fuerte correlaci√≥n con el l√≠mite de cr√©dito, sugiriendo que un mayor l√≠mite de cr√©dito generalmente conlleva una mejor calificaci√≥n crediticia.

4. **Edad y N√∫mero de Tarjetas**: Hay una correlaci√≥n positiva moderada entre estas variables, lo que podr√≠a indicar que las personas mayores suelen tener m√°s tarjetas de cr√©dito.

5. **Valores At√≠picos**: Se observaron valores at√≠picos en las variables de ingresos y l√≠mite de cr√©dito, especialmente para los estudiantes. Estos valores podr√≠an requerir una investigaci√≥n m√°s detallada para entender su origen.

6. **Datos Faltantes**: Un peque√±o porcentaje de datos faltantes en variables como ingresos, l√≠mite de cr√©dito, calificaci√≥n y saldo requiere un tratamiento adecuado antes de cualquier an√°lisis posterior.

7. **Distribuci√≥n Regional**: La distribuci√≥n de individuos entre las diferentes regiones de Colombia es aproximadamente uniforme, lo que podr√≠a facilitar un an√°lisis representativo a nivel nacional.

8. **Estado de Estudiante e Ingresos**: Se observ√≥ que los estudiantes generalmente tienen ingresos m√°s bajos, aunque hay excepciones significativas.

En resumen, el conjunto de datos ofrece una rica variedad de variables para explorar y analizar. No obstante, se requiere una limpieza y preprocesamiento adicionales para tratar los valores nulos y at√≠picos antes de proceder con cualquier modelado o an√°lisis m√°s detallado.
---